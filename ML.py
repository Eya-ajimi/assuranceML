# -*- coding: utf-8 -*-
"""Copie de mini_projet.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Z876EI5PlS_dDC1uyDKoHmL56X50fxcd
"""

#Importation des bibliothèques
import pandas as pd
import seaborn as sns
from sklearn.compose import ColumnTransformer
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
from sklearn.tree import DecisionTreeClassifier, export_text, plot_tree
from sklearn.metrics import classification_report, confusion_matrix
import numpy as np
from sklearn.metrics import ConfusionMatrixDisplay
import matplotlib.pyplot as plt

#Partie 1: Exploration des données

# Supposons que le fichier s'appelle 'dataAssurance.csv'
df_assurance = pd.read_csv('dataAssurance.csv')

df_assurance.head()

#Structure du dataset
df_assurance.columns

#Taille du dataset
df_assurance.shape

#Partie 1: Nettoyage de données

#Gestion des doublons
#1- Détection des doublons
print("Nombre de doublons:", df_assurance.duplicated().sum())
#2- Supprimer les doublons
df_assurance.drop_duplicates(inplace=True)
#3-Vérification de la suppression des doublons
print("Nombre de doublons après suppression:", df_assurance.duplicated().sum())

#Vérification de l'existence des valeurs aberrantes
#Créer des boxplots pour chaque colonne
for column in df_assurance.columns[:-1]: # Exclure la colonne 'charges'
    plt.figure(figsize=(8, 4))
    #sns.boxplot(x=df_assurance[column])
    plt.title(f'Boxplot de {column}')
    plt.xlabel(column)

# Calcul des statistiques pour BMI
Q1 = df_assurance['bmi'].quantile(0.25)
Q3 = df_assurance['bmi'].quantile(0.75)
IQR = Q3 - Q1

lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

print(f"Q1 (25ème percentile): {Q1:.2f}")
print(f"Q3 (75ème percentile): {Q3:.2f}")
print(f"IQR: {IQR:.2f}")
print(f"Borne inférieure: {lower_bound:.2f}")
print(f"Borne supérieure: {upper_bound:.2f}")

# Identifier les outliers
bmi_outliers = df_assurance[(df_assurance['bmi'] < lower_bound) | (df_assurance['bmi'] > upper_bound)]
print(f"\nNombre d'outliers dans BMI: {len(bmi_outliers)}")
print("\nValeurs outliers:")
print(bmi_outliers['bmi'].sort_values())

#Plage d'outliers : [47.41,53.13]
#On conserve les outliers car elles correspondent à des cas d'obésité sévère, qui sont médicalement plausibles.

#Gestion des valeurs manquantes
#1-Détection des valeurs manquantes
df_assurance.isnull().sum()

#2-Imputation des valeurs manquantes
# 1. age - Imputation par la médiane
df_assurance['age'] = df_assurance['age'].fillna(df_assurance['age'].median())

# 2. sex - Imputation par le mode
df_assurance['sex'] = df_assurance['sex'].fillna(df_assurance['sex'].mode()[0])

# 3. bmi - Imputation par la médiane (robuste aux outliers)
df_assurance['bmi'] = df_assurance['bmi'].fillna(df_assurance['bmi'].median())

# 4. children - Imputation par le mode
df_assurance['children'] = df_assurance['children'].fillna(df_assurance['children'].mode()[0])

# 5. smoker - Imputation par le mode
df_assurance['smoker'] = df_assurance['smoker'].fillna(df_assurance['smoker'].mode()[0])

# 6. region - Imputation par le mode
df_assurance['region'] = df_assurance['region'].fillna(df_assurance['region'].mode()[0])

df_assurance.isnull().sum()

#Suppression des lignes contenant la valeur NULL pour la cible
df_assurance = df_assurance.dropna(subset=['charges'])
df_assurance.isnull().sum()

df_assurance.info()

#Partie 2: Transformation des données

#Séparation des variables
X = df_assurance.drop("charges", axis=1)
y = df_assurance["charges"]

from sklearn.preprocessing import OneHotEncoder, MinMaxScaler
# Colonnes par type
categorical_cols = ['sex', 'smoker', 'region']
numeric_cols = ['age', 'bmi', 'children']

# 1. Encodage One-Hot des variables catégorielles
encoder = OneHotEncoder(sparse_output=False)
one_hot_encoded = encoder.fit_transform(X[categorical_cols])

# Création du DataFrame des variables encodées
encoded_df = pd.DataFrame(one_hot_encoded,
                         columns=encoder.get_feature_names_out(categorical_cols))

# 2. Normalisation des variables numériques
scaler = MinMaxScaler()
scaled_numeric = scaler.fit_transform(X[numeric_cols])

# Création du DataFrame des variables normalisées
scaled_df = pd.DataFrame(scaled_numeric, columns=numeric_cols)

# 3. Combinaison des données transformées
X_transformed = pd.concat([scaled_df, encoded_df], axis=1)

# Résultat final
print(X_transformed.head())

#Importance des features sur les charges
from sklearn.ensemble import RandomForestRegressor


# 1. Créer le modèle Random Forest
rf = RandomForestRegressor(n_estimators=100, random_state=42)

# 2. Entraîner le modèle sur les features transformées et la target
rf.fit(X_transformed, y)

# 3. Récupérer l'importance des features
feature_importances = pd.Series(rf.feature_importances_, index=X_transformed.columns)

# 4. Trier les features par importance décroissante
feature_importances = feature_importances.sort_values(ascending=False)

# 5. Afficher les résultats
print(feature_importances)

"""# Objectif DS: Segmentation des assurés"""

import pandas as pd
from sklearn.cluster import KMeans
from sklearn.mixture import GaussianMixture
from sklearn.metrics import silhouette_score
from sklearn.preprocessing import OneHotEncoder, MinMaxScaler
import numpy as np

# ================================
# FEATURE ENGINEERING POUR CLUSTERING
# ================================
df_clustering = df_assurance.copy()
df_clustering['bmi_category'] = pd.cut(df_clustering['bmi'],
                                       bins=[0,18.5,25,30,35,100],
                                       labels=['Sous-poids','Normal','Surpoids','Obésité I','Obésité II'])
df_clustering['age_group'] = pd.cut(df_clustering['age'],
                                    bins=[0,30,45,60,100],
                                    labels=['Jeune','Adulte','Senior','Âgé'])

# ================================
# SÉPARATION FUMEURS / NON-FUMEURS
# ================================
df_fumeurs = df_clustering[df_clustering['smoker']=='yes'].copy()
df_non_fumeurs = df_clustering[df_clustering['smoker']=='no'].copy()

print(f"Fumeurs: {len(df_fumeurs)} ({len(df_fumeurs)/len(df_clustering)*100:.1f}%)")
print(f"Non-fumeurs: {len(df_non_fumeurs)} ({len(df_non_fumeurs)/len(df_clustering)*100:.1f}%)")

# ================================
# PRÉPROCESSING  - SANS SMOKER CAR DÉJÀ SÉPARÉ
# ================================
categorical_cols = ['sex', 'region', 'bmi_category', 'age_group']
numeric_cols = ['age', 'bmi', 'children']

encoder = OneHotEncoder(sparse_output=False)
scaler = MinMaxScaler()

encoder.fit(df_clustering[categorical_cols])
scaler.fit(df_clustering[numeric_cols])
def preprocess_data(df, encoder, scaler):
    categorical_cols = ['sex', 'region', 'bmi_category', 'age_group']
    numeric_cols = ['age', 'bmi', 'children']

    one_hot_encoded = encoder.transform(df[categorical_cols])
    encoded_df = pd.DataFrame(one_hot_encoded, columns=encoder.get_feature_names_out(categorical_cols))

    scaled_numeric = scaler.transform(df[numeric_cols])
    scaled_df = pd.DataFrame(scaled_numeric, columns=numeric_cols)

    X_transformed = pd.concat([scaled_df, encoded_df], axis=1)
    return X_transformed

import pandas as pd
import numpy as np
from sklearn.cluster import KMeans
from sklearn.mixture import GaussianMixture

def clustering_hierarchique(df, features_lvl1, features_lvl2, n_lvl1, n_lvl2, modele='kmeans'):
    """
    Clustering hiérarchique à deux niveaux :
    Niveau 1 -> features_lvl1 (ex: BMI)
    Niveau 2 -> features_lvl2 (ex: âge) à l'intérieur de chaque cluster du niveau 1
    modele: 'kmeans' ou 'gmm'
    """
    df = df.copy()

    # --- Niveau 1 : clustering par features_lvl1 (ex: BMI)
    X1 = df[features_lvl1]
    if modele.lower() == 'kmeans':
        model1 = KMeans(n_clusters=n_lvl1, random_state=42)
        df['cluster_lvl1'] = model1.fit_predict(X1)
    else:
        model1 = GaussianMixture(n_components=n_lvl1, random_state=42)
        df['cluster_lvl1'] = model1.fit_predict(X1)

    # --- Niveau 2 : clustering par features_lvl2 (ex: âge) à l'intérieur de chaque cluster niveau 1
    df['cluster_final'] = -1
    cluster_id = 0

    for lvl1 in df['cluster_lvl1'].unique():
        df_sub = df[df['cluster_lvl1'] == lvl1]
        X2 = df_sub[features_lvl2]

        if len(df_sub) < n_lvl2:
            # Si trop peu d'individus, on garde un seul cluster
            df.loc[df_sub.index, 'cluster_final'] = cluster_id
            cluster_id += 1
        else:
            # Appliquer le modèle choisi
            if modele.lower() == 'kmeans':
                model2 = KMeans(n_clusters=n_lvl2, random_state=42)
                labels2 = model2.fit_predict(X2)
            else:
                model2 = GaussianMixture(n_components=n_lvl2, random_state=42)
                labels2 = model2.fit_predict(X2)

            # Assignation des sous-clusters correctement selon l'index original
            for l in np.unique(labels2):
                mask = (labels2 == l)
                df.loc[df_sub.index[mask], 'cluster_final'] = cluster_id
                cluster_id += 1

    return df

# ================================
# APPLICATION DU CLUSTERING HIÉRARCHIQUE
# ================================

# Pour les non-fumeurs
df_non_fumeurs = clustering_hierarchique(
    df_non_fumeurs,
    features_lvl1=['bmi'],     # Niveau 1 : BMI
    features_lvl2=['age'],     # Niveau 2 : âge
    n_lvl1=3,
    n_lvl2=2,
    modele='kmeans'
)

# Pour les fumeurs
df_fumeurs = clustering_hierarchique(
    df_fumeurs,
    features_lvl1=['bmi'],     # Niveau 1 : BMI
    features_lvl2=['age'],     # Niveau 2 : âge
    n_lvl1=2,
    n_lvl2=2,
    modele='gmm'
)

# Vérification
print("Non-fumeurs :")
print(df_non_fumeurs[['bmi', 'age', 'cluster_lvl1', 'cluster_final']].head(10))

print("\nFumeurs :")
print(df_fumeurs[['bmi', 'age', 'cluster_lvl1', 'cluster_final']].head(10))

# ================================
# FONCTION POUR CHOISIR LE MEILLEUR MODÈLE
# ================================
def calcul_score_silhouette(X_data, kmeans_model, gmm_model, labels_kmeans, labels_gmm):
    """
    Calcule les scores de silhouette et choisit le meilleur modèle
    """
    print(f"\n{'='*60}")
    print("COMPARAISON DES MODÈLES - SILHOUETTE SCORE")
    print(f"{'='*60}")

    # Calcul des scores de silhouette
    silhouette_kmeans = silhouette_score(X_data, labels_kmeans)
    silhouette_gmm = silhouette_score(X_data, labels_gmm)

    print(f"📊 Scores de silhouette:")
    print(f"   KMeans: {silhouette_kmeans:.4f}")
    print(f"   GMM:     {silhouette_gmm:.4f}")

    # Choix du meilleur modèle
    if silhouette_kmeans > silhouette_gmm:
        meilleur_modele = "KMEANS"
        meilleur_score = silhouette_kmeans
        print(f"🎯 MEILLEUR MODÈLE: KMeans (score: {meilleur_score:.4f})")
    else:
        meilleur_modele = "GMM"
        meilleur_score = silhouette_gmm
        print(f"🎯 MEILLEUR MODÈLE: GMM (score: {meilleur_score:.4f})")

    return meilleur_modele, meilleur_score, kmeans_model, gmm_model

# ================================
# CLUSTERING DÉTAILLÉ DES NON-FUMEURS AVEC 2 MODÈLES
# ================================
print(f"\n{'='*80}")
print("CLUSTERING DÉTAILLÉ DES NON-FUMEURS")
print(f"{'='*80}")

X_non_fumeurs = preprocess_data(df_non_fumeurs, encoder, scaler)
print(f" Données non-fumeurs transformées: {X_non_fumeurs.shape}")

# Recherche du k optimal pour non-fumeurs
silhouette_scores_nf = []
K_range_nf = range(2, 6)

for k in K_range_nf:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    cluster_labels = kmeans.fit_predict(X_non_fumeurs)
    silhouette_scores_nf.append(silhouette_score(X_non_fumeurs, cluster_labels))
    print(f"K={k} - Silhouette: {silhouette_scores_nf[-1]:.3f}")

optimal_k_nf = K_range_nf[silhouette_scores_nf.index(max(silhouette_scores_nf))]
print(f" K optimal pour non-fumeurs: {optimal_k_nf}")

# Création des DEUX modèles pour non-fumeurs
kmeans_nf = KMeans(n_clusters=optimal_k_nf, random_state=42, n_init=10)
gmm_nf = GaussianMixture(n_components=optimal_k_nf, random_state=42)

# Entraînement des deux modèles
clusters_kmeans_nf = kmeans_nf.fit_predict(X_non_fumeurs)
clusters_gmm_nf = gmm_nf.fit_predict(X_non_fumeurs)

# Choix du meilleur modèle pour non-fumeurs
meilleur_modele_nf, score_nf, kmeans_model_nf, gmm_model_nf = calcul_score_silhouette(
    X_non_fumeurs, kmeans_nf, gmm_nf, clusters_kmeans_nf, clusters_gmm_nf
)

# Assigner les clusters du meilleur modèle
if meilleur_modele_nf == "KMEANS":
    df_non_fumeurs['sous_cluster'] = clusters_kmeans_nf
    modele_final_nf = kmeans_model_nf
else:
    df_non_fumeurs['sous_cluster'] = clusters_gmm_nf
    modele_final_nf = gmm_model_nf

print(f" {optimal_k_nf} sous-clusters créés pour les non-fumeurs avec {meilleur_modele_nf}")

# ================================
# CLUSTERING DÉTAILLÉ DES FUMEURS AVEC 2 MODÈLES
# ================================
print(f"\n{'='*80}")
print("CLUSTERING DÉTAILLÉ DES FUMEURS")
print(f"{'='*80}")

X_fumeurs = preprocess_data(df_fumeurs, encoder, scaler)
print(f" Données fumeurs transformées: {X_fumeurs.shape}")

# Recherche du k optimal pour fumeurs
silhouette_scores_f = []
K_range_f = range(2, min(5, len(df_fumeurs) - 1))

for k in K_range_f:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    cluster_labels = kmeans.fit_predict(X_fumeurs)
    silhouette_scores_f.append(silhouette_score(X_fumeurs, cluster_labels))
    print(f"K={k} - Silhouette: {silhouette_scores_f[-1]:.3f}")

optimal_k_f = K_range_f[silhouette_scores_f.index(max(silhouette_scores_f))]
print(f" K optimal pour fumeurs: {optimal_k_f}")

# Création des DEUX modèles pour fumeurs
kmeans_f = KMeans(n_clusters=optimal_k_f, random_state=42, n_init=10)
gmm_f = GaussianMixture(n_components=optimal_k_f, random_state=42)

# Entraînement des deux modèles
clusters_kmeans_f = kmeans_f.fit_predict(X_fumeurs)
clusters_gmm_f = gmm_f.fit_predict(X_fumeurs)

# Choix du meilleur modèle pour fumeurs
meilleur_modele_f, score_f, kmeans_model_f, gmm_model_f = calcul_score_silhouette(
    X_fumeurs, kmeans_f, gmm_f, clusters_kmeans_f, clusters_gmm_f
)

# Assigner les clusters du meilleur modèle
if meilleur_modele_f == "KMEANS":
    df_fumeurs['sous_cluster'] = clusters_kmeans_f
    modele_final_f = kmeans_model_f
else:
    df_fumeurs['sous_cluster'] = clusters_gmm_f
    modele_final_f = gmm_model_f

print(f" {optimal_k_f} sous-clusters créés pour les fumeurs avec {meilleur_modele_f}")

def analyser_sous_clusters(df, type_client):
    cluster_stats = []
    for cluster_id in sorted(df['cluster_final'].unique()):
        data = df[df['cluster_final']==cluster_id]
        avg_bmi = data['bmi'].mean()
        avg_age = data['age'].mean()
        avg_charges = data['charges'].mean()
        avg_children = data['children'].mean()

        # Profil BMI + âge
        if avg_bmi>35:
            bmi_label = "Obésité Sévère"
        elif avg_bmi>30:
            bmi_label = "Obésité"
        elif avg_bmi>25:
            bmi_label = "Surpoids"
        else:
            bmi_label = "Poids Normal"
        if avg_age>55:
            age_label = "Seniors"
        elif avg_age>40:
            age_label = "Adultes"
        else:
            age_label = "Jeunes"

        nom = f"{type_client} - {bmi_label} + {age_label}"
        cluster_stats.append({
            'cluster_id': cluster_id,
            'nom': nom,
            'avg_bmi': avg_bmi,
            'avg_age': avg_age,
            'avg_charges': avg_charges,
            'avg_children': avg_children,
            'count': len(data),
            'type_client': type_client
        })
    return cluster_stats

sous_clusters_nf = analyser_sous_clusters(df_non_fumeurs,"NON-FUMEURS")
sous_clusters_f = analyser_sous_clusters(df_fumeurs,"FUMEURS")

# Mapping des noms
noms_sous_clusters_nf = {stat['cluster_id']:stat['nom'] for stat in sous_clusters_nf}
noms_sous_clusters_f = {stat['cluster_id']:stat['nom'] for stat in sous_clusters_f}

df_non_fumeurs['nom_sous_cluster'] = df_non_fumeurs['cluster_final'].map(noms_sous_clusters_nf)
df_fumeurs['nom_sous_cluster'] = df_fumeurs['cluster_final'].map(noms_sous_clusters_f)

# ================================
# FUSION DES RÉSULTATS DANS LE DATAFRAME PRINCIPAL
# ================================
df_clustering['sous_cluster'] = -1
df_clustering['nom_sous_cluster'] = "Non assigné"
df_clustering['type_cluster'] = "Non assigné"

for idx in df_non_fumeurs.index:
    df_clustering.at[idx,'sous_cluster'] = df_non_fumeurs.at[idx,'cluster_final']
    df_clustering.at[idx,'nom_sous_cluster'] = df_non_fumeurs.at[idx,'nom_sous_cluster']
    df_clustering.at[idx,'type_cluster'] = 'NON-FUMEURS'

for idx in df_fumeurs.index:
    df_clustering.at[idx,'sous_cluster'] = df_fumeurs.at[idx,'cluster_final']
    df_clustering.at[idx,'nom_sous_cluster'] = df_fumeurs.at[idx,'nom_sous_cluster']
    df_clustering.at[idx,'type_cluster'] = 'FUMEURS'

def profiling_sous_clusters_detaille(sous_clusters, type_client):
    print(f"\n{'='*80}\nPROFILING DÉTAILLÉ - {type_client}\n{'='*80}")
    for stat in sous_clusters:
        print(f"\n🔹 Sous-cluster {stat['cluster_id']}")
        print(f"   📋 {stat['nom']}")
        print(f"   👥 {stat['count']} clients ({stat['count']/len(df_clustering)*100:.1f}%)")
        print(f"   📊 Âge: {stat['avg_age']:.1f} ans | BMI: {stat['avg_bmi']:.1f} | Enfants: {stat['avg_children']:.1f}")
        print(f"   💰 Charges moyennes: ${stat['avg_charges']:,.0f}")

profiling_sous_clusters_detaille(sous_clusters_nf,"NON-FUMEURS")
profiling_sous_clusters_detaille(sous_clusters_f,"FUMEURS")

def preprocess_client_data_detaille(df_client):
    categorical_cols = ['sex','region','bmi_category','age_group']
    numeric_cols = ['age','bmi','children']
    df_client = df_client.copy()
    df_client['bmi_category'] = pd.cut(df_client['bmi'], bins=[0,18.5,25,30,35,100], labels=['Sous-poids','Normal','Surpoids','Obésité I','Obésité II'])
    df_client['age_group'] = pd.cut(df_client['age'], bins=[0,30,45,60,100], labels=['Jeune','Adulte','Senior','Âgé'])
    one_hot = encoder.transform(df_client[categorical_cols])
    one_hot_df = pd.DataFrame(one_hot, columns=encoder.get_feature_names_out(categorical_cols))
    scaled = scaler.transform(df_client[numeric_cols])
    scaled_df = pd.DataFrame(scaled, columns=numeric_cols)
    return pd.concat([scaled_df, one_hot_df], axis=1)

def predict_cluster_detaille(df_client, df_nf, df_f):
    df_client_proc = preprocess_client_data_detaille(df_client)
    if df_client['smoker'].iloc[0]=='yes':
        # Prend le cluster le plus proche par BMI puis âge
        df_temp = df_f.copy()
    else:
        df_temp = df_nf.copy()
    # Calcul distances euclidiennes pour cluster_final
    cluster_ids = df_temp['cluster_final'].unique()
    distances = []
    for c in cluster_ids:
        df_c = df_temp[df_temp['cluster_final']==c]
        centroid_bmi = df_c['bmi'].mean()
        centroid_age = df_c['age'].mean()
        dist = np.sqrt((df_client['bmi'].iloc[0]-centroid_bmi)**2 + (df_client['age'].iloc[0]-centroid_age)**2)
        distances.append(dist)
    cluster_id = cluster_ids[np.argmin(distances)]
    if df_client['smoker'].iloc[0]=='yes':
        nom_cluster = noms_sous_clusters_f.get(cluster_id,"Inconnu")
        type_client = "FUMEURS"
    else:
        nom_cluster = noms_sous_clusters_nf.get(cluster_id,"Inconnu")
        type_client = "NON-FUMEURS"
    return cluster_id, nom_cluster, type_client

# =============================================================================
# Déterminer le pack
# =============================================================================
def definir_pack_auto(avg_bmi, avg_age, type_client):
    """
    Détermine automatiquement un pack attractif en fonction de :
    - avg_bmi : BMI moyen du cluster
    - avg_age : âge moyen du cluster
    - type_client : 'FUMEURS' ou 'NON-FUMEURS'
    """
    # Définir le niveau du pack selon BMI
    if avg_bmi > 35:
        niveau_pack = "VIP"
    elif avg_bmi > 30:
        niveau_pack = "Premium Plus"
    elif avg_bmi > 25:
        niveau_pack = "Premium"
    else:
        niveau_pack = "Standard"

    # Optionnel : suffixe selon âge
    if avg_age > 55:
        age_suffix = "Senior"
    elif avg_age > 40:
        age_suffix = "Adulte"
    else:
        age_suffix = "Jeune"

    # Construction du nom attractif
    pack = f"{niveau_pack} "

    return pack
# NON-FUMEURS
packs_nf_auto = {stat['cluster_id']: definir_pack_auto(stat['avg_bmi'], stat['avg_age'], "NON-FUMEURS")
                 for stat in sous_clusters_nf}

# FUMEURS
packs_f_auto = {stat['cluster_id']: definir_pack_auto(stat['avg_bmi'], stat['avg_age'], "FUMEURS")
                for stat in sous_clusters_f}

# Mise à jour des DataFrames
df_non_fumeurs['pack'] = df_non_fumeurs['cluster_final'].map(packs_nf_auto)
df_fumeurs['pack'] = df_fumeurs['cluster_final'].map(packs_f_auto)

def evaluer_risque_client_detaille(df_client, df_nf, df_f):
    cluster_id, nom_cluster, type_client = predict_cluster_detaille(df_client, df_nf, df_f)

    charges_moyennes = df_clustering[df_clustering['sous_cluster']==cluster_id]['charges'].mean()

    if charges_moyennes > 15000:
        risque = "Élevé"
    elif charges_moyennes > 8000:
        risque = "Moyen"
    else:
        risque = "Faible"

    # Pack automatique
    if type_client == "FUMEURS":
        pack = packs_f_auto.get(cluster_id, "Inconnu")
    else:
        pack = packs_nf_auto.get(cluster_id, "Inconnu")

    return {
        'cluster_id': cluster_id,
        'type_client': type_client,
        'nom_cluster': nom_cluster,
        'charges_moyennes': charges_moyennes,
        'risque': risque,
        'pack': pack,
        'profil': profil_individuel_client(df_client)
    }

# =============================================================================
# PRÉDICTION CLIENT
# =============================================================================
def profil_individuel_client(client_data):
    bmi = client_data['bmi'].iloc[0]
    age = client_data['age'].iloc[0]
    if bmi>35: bmi_label="Obésité Sévère"
    elif bmi>30: bmi_label="Obésité"
    elif bmi>25: bmi_label="Surpoids"
    else: bmi_label="Poids Normal"
    if age>55: age_label="Seniors"
    elif age>40: age_label="Adultes"
    else: age_label="Jeunes"
    return f"{bmi_label} + {age_label}"

# -----------------------------
#  Création de clients exemples
# -----------------------------
clients_exemples = pd.DataFrame([
    {'age': 28, 'bmi': 22, 'children': 0, 'sex': 'male', 'smoker': 'no', 'region': 'northwest'},
    {'age': 45, 'bmi': 27, 'children': 2, 'sex': 'female', 'smoker': 'yes', 'region': 'southeast'},
    {'age': 60, 'bmi': 31, 'children': 3, 'sex': 'male', 'smoker': 'no', 'region': 'northeast'},
    {'age': 35, 'bmi': 36, 'children': 1, 'sex': 'female', 'smoker': 'yes', 'region': 'southwest'}
])

# -----------------------------
# Tester le clustering détaillé et afficher le pack
# -----------------------------
for i, client in clients_exemples.iterrows():
    client_df = pd.DataFrame([client])

    # Prédiction du cluster
    cluster_id, nom_cluster, type_client = predict_cluster_detaille(client_df, df_non_fumeurs, df_fumeurs)

    # Évaluation du risque (renvoie aussi le pack)
    resultat_risque = evaluer_risque_client_detaille(client_df, df_non_fumeurs, df_fumeurs)

    print(f"\nClient {i+1}:")
    print(f"  Type client: {type_client}")
    print(f"  Cluster ID: {cluster_id}")
    print(f"  Nom du sous-cluster: {nom_cluster}")
    print(f"  Pack attribué: {resultat_risque['pack']}")
    print(f"  Charges moyennes: {resultat_risque['charges_moyennes']}")
    print(f"  Risque: {resultat_risque['risque']}")

"""#Objectif DS: Construire les 2 modèles de régression pour prédire les frais médicaux (charges) et comparer deux modèles : Régression Linéaire vs XGBoost"""

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import xgboost as xgb
from typing import Tuple, Dict, Any

#Pour de meilleurs visuels
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")

print(X_transformed.head())

X = X_transformed
y = df_assurance["charges"]
print("Dimensions de X :", X.shape)
print("Dimensions de y :", y.shape)

# Split Train/Test (une seule fois pour les deux modèles)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

print("Taille du jeu d’entraînement :", X_train.shape)
print("Taille du jeu de test :", X_test.shape)

"""
*   Fonction evaluate_model va évaluer un modèle et retourne les métriques

*   Visualisation les prédictions vs valeurs réelles

*   Création d'un DataFrame de comparaison à partir d'un dictionnaire de métriques"""

def evaluate_model(y_true: np.ndarray, y_pred: np.ndarray, model_name: str) -> Dict[str, float]:
    """
    Évalue un modèle et retourne les métriques
    """
    metrics = {
        'MAE': mean_absolute_error(y_true, y_pred),
        'RMSE': np.sqrt(mean_squared_error(y_true, y_pred)),
        'R2': r2_score(y_true, y_pred),
        'MAPE': np.mean(np.abs((y_true - y_pred) / y_true)) * 100  # Erreur en pourcentage
    }

    print(f"\n📊 Évaluation {model_name} :")
    for metric, value in metrics.items():
        if metric == 'MAPE':
            print(f"{metric:5} : {value:.2f}%")
        else:
            print(f"{metric:5} : {value:.2f}")

    return metrics

def plot_predictions(y_true: np.ndarray, y_pred: np.ndarray, model_name: str):
    """
    Visualise les prédictions vs valeurs réelles
    """
    fig, axes = plt.subplots(1, 2, figsize=(15, 6))

    # Graphique de dispersion
    axes[0].scatter(y_true, y_pred, alpha=0.6, s=50)
    axes[0].plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--', lw=2)
    axes[0].set_xlabel("Charges réelles")
    axes[0].set_ylabel("Charges prédites")
    axes[0].set_title(f"{model_name} : Prédictions vs Réelles")
    axes[0].grid(True, alpha=0.3)

    # Distribution des erreurs
    errors = y_true - y_pred
    sns.histplot(errors, kde=True, ax=axes[1])
    axes[1].axvline(x=0, color='r', linestyle='--')
    axes[1].set_title(f"{model_name} : Distribution des erreurs")
    axes[1].set_xlabel("Erreur de prédiction")

    plt.tight_layout()
    plt.show()

    return errors

def create_comparison_df(metrics_dict: Dict[str, Dict]) -> pd.DataFrame:
    """
    Crée un DataFrame de comparaison à partir d'un dictionnaire de métriques
    """
    comparison_data = []
    for model_name, metrics in metrics_dict.items():
        row = {'Modèle': model_name}
        row.update(metrics)
        comparison_data.append(row)

    return pd.DataFrame(comparison_data)

"""# Modèle 1 : Régression Linéaire

"""

# Modèle 1 : Régression Linéaire

print("🔧 Entraînement du modèle de Régression Linéaire...")

# Construction et entraînement du modèle
model_lr = LinearRegression()
model_lr.fit(X_train, y_train)

# Prédictions
y_pred_lr = model_lr.predict(X_test)

# Évaluation avec la fonction
metrics_lr = evaluate_model(y_test, y_pred_lr, "Régression Linéaire")

# Visualisations
errors_lr = plot_predictions(y_test, y_pred_lr, "Régression Linéaire")

# Informations supplémentaires
print(f"\n📈 Coefficients du modèle linéaire : {len(model_lr.coef_)}")
print(f"📍 Intercept : {model_lr.intercept_:.2f}")

"""#Modèle 2 : XGBoost

"""

#Modèle 2 : XGBoost

print("\n🔧 Entraînement du modèle XGBoost...")

# Conversion en DMatrix
dtrain = xgb.DMatrix(X_train, label=y_train)
dtest = xgb.DMatrix(X_test, label=y_test)

# Paramètres avec validation croisée implicite
params = {
    "objective": "reg:squarederror",
    "tree_method": "hist",
    "learning_rate": 0.1,
    "max_depth": 6,
    "subsample": 0.8,
    "colsample_bytree": 0.8,
    "reg_alpha": 0.1,  # L1 regularization
    "reg_lambda": 1.0, # L2 regularization
    "seed": 42
}

# Entraînement avec callback de progression
evals = [(dtrain, "train"), (dtest, "validation")]

print("⏳ Début de l'entraînement XGBoost...")
model_xgb = xgb.train(
    params=params,
    dtrain=dtrain,
    num_boost_round=1000,
    evals=evals,
    early_stopping_rounds=50,
    verbose_eval=50
)

# Prédictions
y_pred_xgb = model_xgb.predict(dtest)

# Évaluation
metrics_xgb = evaluate_model(y_test, y_pred_xgb, "XGBoost")

# Visualisations
errors_xgb = plot_predictions(y_test, y_pred_xgb, "XGBoost")

# Feature importance
plt.figure(figsize=(3, 8))
xgb.plot_importance(model_xgb, max_num_features=15, importance_type='weight')
plt.title("XGBoost - Importance des caractéristiques")
plt.tight_layout()
plt.show()

"""# Comparaison des 2 algorithmes

"""

# Création du DataFrame de comparaison
metrics_dict = {
    "Régression Linéaire": metrics_lr,
    "XGBoost": metrics_xgb
}

results_df = create_comparison_df(metrics_dict)

print("\n" + "="*50)
print("📊 RÉSULTATS COMPARATIFS DÉTAILLÉS")
print("="*50)
print(results_df.round(4))

# Visualisation comparative améliorée
fig, axes = plt.subplots(2, 2, figsize=(8, 8))

# Graphique des métriques
metrics_to_plot = ['MAE', 'RMSE', 'R2', 'MAPE']
for idx, metric in enumerate(metrics_to_plot):
    ax = axes[idx//2, idx%2]
    sns.barplot(data=results_df, x='Modèle', y=metric, ax=ax, palette='viridis')
    ax.set_title(f'Comparaison - {metric}')
    ax.tick_params(axis='x', rotation=45)

    # Ajout des valeurs sur les barres
    for container in ax.containers:
        ax.bar_label(container, fmt='%.3f')

plt.tight_layout()
plt.show()

# Graphique des erreurs résiduelles
plt.figure(figsize=(10, 6))
plt.subplot(1, 2, 1)
sns.boxplot(data=[errors_lr, errors_xgb], palette='Set2')
plt.xticks([0, 1], ['Régression Linéaire', 'XGBoost'])
plt.title('Distribution des erreurs résiduelles')
plt.ylabel('Erreur')

plt.subplot(1, 2, 2)
sns.kdeplot(errors_lr, label='Régression Linéaire', fill=True)
sns.kdeplot(errors_xgb, label='XGBoost', fill=True)
plt.axvline(x=0, color='red', linestyle='--', alpha=0.5)
plt.title('Densité des erreurs')
plt.xlabel('Erreur')
plt.legend()

plt.tight_layout()
plt.show()

"""#Analyse de performance supplémentaire entre les 2 algorithmes

"""

#Analyse de performance supplémentaire

def analyze_residuals(y_true: np.ndarray, y_pred: np.ndarray, model_name: str):
    """Analyse approfondie des résidus"""
    residuals = y_true - y_pred

    fig, axes = plt.subplots(2, 2, figsize=(12, 10))

    # Résidus vs prédictions
    axes[0,0].scatter(y_pred, residuals, alpha=0.6)
    axes[0,0].axhline(y=0, color='red', linestyle='--')
    axes[0,0].set_xlabel('Prédictions')
    axes[0,0].set_ylabel('Résidus')
    axes[0,0].set_title(f'{model_name} - Résidus vs Prédictions')

    # QQ plot pour normalité
    from scipy import stats
    stats.probplot(residuals, dist="norm", plot=axes[0,1])
    axes[0,1].set_title(f'{model_name} - QQ Plot')

    # Autocorrélation des résidus
    from statsmodels.tsa.stattools import acf
    acf_residuals = acf(residuals, nlags=40)
    axes[1,0].stem(acf_residuals)
    axes[1,0].set_title(f'{model_name} - Autocorrélation des résidus')
    axes[1,0].set_xlabel('Lag')
    axes[1,0].set_ylabel('Autocorrélation')

    # Histogramme des résidus
    axes[1,1].hist(residuals, bins=30, density=True, alpha=0.7)
    axes[1,1].set_xlabel('Résidus')
    axes[1,1].set_ylabel('Densité')
    axes[1,1].set_title(f'{model_name} - Distribution des résidus')

    plt.tight_layout()
    plt.show()

# Application aux deux modèles
analyze_residuals(y_test, y_pred_lr, "Régression Linéaire")
analyze_residuals(y_test, y_pred_xgb, "XGBoost")

"""# Identification du meilleur modèle

"""

# Identification du meilleur modèle

# Access metrics from the dictionaries
mae_xgb = metrics_xgb['MAE']
mae_lr = metrics_lr['MAE']
r2_xgb = metrics_xgb['R2']
r2_lr = metrics_lr['R2']


if mae_xgb < mae_lr and r2_xgb > r2_lr:
    meilleur_modele = "XGBoost"
    modele_final = model_xgb
    print("🎯 CONCLUSION : XGBoest est sélectionné comme modèle final")
    print("   ✓ Plus faible MAE (erreur moyenne)")
    print("   ✓ Meilleur R² (explication de la variance)")
else:
    meilleur_modele = "Régression Linéaire"
    modele_final = model_lr
    print("🎯 CONCLUSION : Régression Linéaire est sélectionnée comme modèle final")

print(f"📊 Amélioration du MAE : {((mae_lr - mae_xgb) / mae_lr * 100):.1f}%")
print(f"📊 Amélioration du R² : {((r2_xgb - r2_lr) / r2_lr * 100):.1f}%")

"""#  PRÉDICTION SUR DE NOUVEAUX CLIENTS

"""

#  PRÉDICTION SUR DE NOUVEAUX CLIENTS

# 1. NOUVEAUX CLIENTS
nouveaux_clients = pd.DataFrame({
    'age': [25, 45, 60, 19, 35],
    'bmi': [22.5, 28.9, 33.1, 19.2, 26.5],
    'children': [0, 2, 1, 0, 2],
    'sex_female': [0, 1, 0, 1, 0],
    'sex_male': [1, 0, 1, 0, 1],
    'smoker_no': [1, 0, 0, 1, 1],
    'smoker_yes': [0, 1, 1, 0, 0],
    'region_northeast': [0, 1, 0, 0, 0],
    'region_northwest': [0, 0, 0, 1, 0],
    'region_southeast': [0, 0, 1, 0, 0],
    'region_southwest': [1, 0, 0, 0, 1]
})

print("👥 PROFILS DES NOUVEAUX CLIENTS :")
print(nouveaux_clients[['age', 'bmi', 'children', 'sex_male', 'smoker_yes']])

# 2. PRÉDICTION AVEC LE MEILLEUR MODÈLE
predictions = modele_final.predict(xgb.DMatrix(nouveaux_clients))
nouveaux_clients['frais_predits'] = predictions

# 3. ANALYSE DES RÉSULTATS (NOUVEAU)
print("\n" + "="*60)
print("💰 RÉSULTATS DES PRÉDICTIONS - ANALYSE DÉTAILLÉE")
print("="*60)

for i in range(len(nouveaux_clients)):
    client = nouveaux_clients.iloc[i]

    # Catégorisation du risque
    if client['smoker_yes'] == 1:
        risque = "🔴 RISQUE ÉLEVÉ"
        motif = "(Fumeur)"
    elif client['bmi'] > 30:
        risque = "🟡 RISQUE MODÉRÉ"
        motif = "(Obésité)"
    elif client['age'] > 50:
        risque = "🟠 RISQUE MOYEN"
        motif = "(Âge avancé)"
    else:
        risque = "🟢 RISQUE FAIBLE"
        motif = "(Profil sain)"

    print(f"👤 Client {i+1}:")
    print(f"   • 📊 Profil: {client['age']} ans, BMI {client['bmi']}, {client['children']} enfant(s)")
    print(f"   • 🚬 Fumeur: {'Oui' if client['smoker_yes'] == 1 else 'Non'}")
    print(f"   • 💰 Frais prédits: ${client['frais_predits']:,.2f}")
    print(f"   • ⚠️  Niveau de risque: {risque} {motif}")
    print("-" * 50)

# 4. STATISTIQUES GLOBALES (NOUVEAU)
print("\n📈 STATISTIQUES GLOBALES DES PRÉDICTIONS :")
print(f"• 📊 Frais moyen: ${nouveaux_clients['frais_predits'].mean():,.2f}")
print(f"• 📉 Frais minimum: ${nouveaux_clients['frais_predits'].min():,.2f}")
print(f"• 📈 Frais maximum: ${nouveaux_clients['frais_predits'].max():,.2f}")
print(f"• 🔄 Écart type: ${nouveaux_clients['frais_predits'].std():,.2f}")

# 5. VISUALISATION DES PRÉDICTIONS (NOUVEAU)
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
colors = ['red' if x == 1 else 'green' for x in nouveaux_clients['smoker_yes']]
plt.bar([f'Client {i+1}' for i in range(len(nouveaux_clients))],
        nouveaux_clients['frais_predits'], color=colors, alpha=0.7)
plt.title('Frais médicaux prédits par client\n(Rouge=Fumeur, Vert=Non-fumeur)')
plt.xticks(rotation=45)
plt.ylabel('Frais ($)')

plt.subplot(1, 2, 2)
plt.scatter(nouveaux_clients['age'], nouveaux_clients['frais_predits'],
           c=nouveaux_clients['bmi'], s=100, cmap='viridis')
plt.colorbar(label='BMI')
plt.xlabel('Âge')
plt.ylabel('Frais prédits ($)')
plt.title('Impact de l\'âge et du BMI')

plt.tight_layout()
plt.show()

"""# Application Dash"""



"""**Classification**

*KNN*
"""

quantiles = df_assurance['charges'].quantile([0.33, 0.66]).values

def assign_reimbursement_class(charges):
    if charges <= quantiles[0]:
        return "R3"  # Faible charges → fort remboursement
    elif charges <= quantiles[1]:
        return "R2"  # Charges moyennes → remboursement moyen
    else:
        return "R1"  # Charges élevées → faible remboursement

df_assurance['remboursement_class'] = df_assurance['charges'].apply(assign_reimbursement_class)

print("Répartition des classes :")
print(df_assurance['remboursement_class'].value_counts())

X = X_transformed.values  # tes features déjà encodées + normalisées
y = df_assurance['remboursement_class'].values  # ta cible

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

knn = KNeighborsClassifier(n_neighbors=5)

knn.fit(X_train, y_train)

y_pred = knn.predict(X_test)

print("Rapport de classification (KNN k=5):\n")
print(classification_report(y_test, y_pred))

cm = confusion_matrix(y_test, y_pred, labels=["R1","R2","R3"])
ConfusionMatrixDisplay(cm, display_labels=["R1","R2","R3"]).plot(cmap=plt.cm.Blues)
plt.title("Matrice de confusion - KNN (k=5)")
plt.show()

# 5) (Optionnel) Chercher le meilleur k
scores = []
ks = range(1, 21)
for k in ks:
    m = KNeighborsClassifier(n_neighbors=k)
    m.fit(X_train, y_train)
    scores.append(m.score(X_test, y_test))

plt.figure()
plt.plot(ks, scores, marker='o')
plt.xlabel("Nombre de voisins (k)")
plt.ylabel("Accuracy sur test")
plt.title("Choix du meilleur k (avec X_transformed)")
plt.show()

"""Arbre

"""

X = X_transformed   # Ton jeu de données encodé et normalisé
y = df_assurance["remboursement_class"]

# Vérifier les dimensions
X.shape, y.shape

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# --- 3. Entraîner l’arbre de décision
clf = DecisionTreeClassifier(
    max_depth=5,
    class_weight={'R1':1, 'R2':1, 'R3':1.5}, # pondération des classes
    random_state=42
)
clf.fit(X_train, y_train)

# Prédictions
y_pred = clf.predict(X_test)

# Matrice de confusion
print("Matrice de confusion :\n", confusion_matrix(y_test, y_pred))

# Rapport de classification
print("\nRapport de classification :\n", classification_report(y_test, y_pred))

ConfusionMatrixDisplay.from_estimator(clf, X_test, y_test, cmap='Blues')
plt.show()

import seaborn as sns
from sklearn.metrics import classification_report

report = classification_report(y_test, y_pred, output_dict=True)
df_report = pd.DataFrame(report).iloc[:-1, :3]  # exclut accuracy

sns.heatmap(df_report, annot=True, cmap="Blues")
plt.title("Precision / Recall / F1-score")
plt.show()

encoded_cols = encoder.get_feature_names_out(categorical_cols)
all_features = numeric_cols + list(encoded_cols)

plt.figure(figsize=(20,10))
plot_tree(clf, feature_names=all_features, class_names=['R1','R2','R3'], filled=True)
plt.show()











#afficahge ////////////////////////////////////////////////

# =============================================================================
# SAUVEGARDE DES MODÈLES POUR FLASK
# =============================================================================

# Sauvegarde des modèles
import joblib
import os
os.makedirs('models', exist_ok=True)
joblib.dump(modele_final, 'models/modele_final.pkl')
joblib.dump(encoder, 'models/encoder.pkl')
joblib.dump(scaler, 'models/scaler.pkl')
joblib.dump(clf, 'models/clf.pkl')
print("✅ Modèles sauvegardés!")


###

# =============================================================================
# SAUVEGARDE DES MODÈLES POUR FLASK
# =============================================================================

def sauvegarder_modeles():
    """Sauvegarde tous les modèles nécessaires pour l'application Flask"""
    import joblib
    import os
    
    # Créer le dossier models s'il n'existe pas
    os.makedirs('models', exist_ok=True)
    
    try:
        # Vérifier que les modèles existent
        required_models = {
            'modele_final': modele_final,
            'encoder': encoder,
            'scaler': scaler,
            'clf': clf
        }
        
        for name, model in required_models.items():
            if model is None:
                print(f"❌ Le modèle {name} n'est pas défini")
                return False
        
        # Sauvegarder les modèles et transformateurs
        joblib.dump(modele_final, 'models/modele_final.pkl')
        joblib.dump(encoder, 'models/encoder.pkl')
        joblib.dump(scaler, 'models/scaler.pkl')
        joblib.dump(clf, 'models/clf.pkl')
        
        print("✅ Modèles sauvegardés avec succès dans le dossier 'models/'")
        print("📁 Fichiers créés :")
        print("   - modele_final.pkl")
        print("   - encoder.pkl") 
        print("   - scaler.pkl")
        print("   - clf.pkl")
        return True
        
    except Exception as e:
        print(f"❌ Erreur lors de la sauvegarde : {e}")
        return False

# Exécuter la sauvegarde si ce script est exécuté directement
if __name__ == "__main__":
    # Attendre que tous les modèles soient créés dans votre code principal
    # Cette partie doit être appelée après l'entraînement de tous les modèles
    
    # Pour l'instant, exécutons juste le nettoyage et l'analyse
    print("🔍 Analyse des données terminée!")
    
    # Si vous voulez sauvegarder immédiatement, décommentez la ligne suivante :
    # sauvegarder_modeles()