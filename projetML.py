# -*- coding: utf-8 -*-
"""Copie de mini_projet.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Z876EI5PlS_dDC1uyDKoHmL56X50fxcd
"""

#Importation des biblioth√®ques
import pandas as pd
import seaborn as sns
from sklearn.compose import ColumnTransformer
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
from sklearn.tree import DecisionTreeClassifier, export_text, plot_tree
from sklearn.metrics import classification_report, confusion_matrix
import numpy as np
from sklearn.metrics import ConfusionMatrixDisplay
import matplotlib.pyplot as plt

#Partie 1: Exploration des donn√©es

# Supposons que le fichier s'appelle 'dataAssurance.csv'
df_assurance = pd.read_csv('dataAssurance.csv')

df_assurance.head()

#Structure du dataset
df_assurance.columns

#Taille du dataset
df_assurance.shape

#Partie 1: Nettoyage de donn√©es

#Gestion des doublons
#1- D√©tection des doublons
print("Nombre de doublons:", df_assurance.duplicated().sum())
#2- Supprimer les doublons
df_assurance.drop_duplicates(inplace=True)
#3-V√©rification de la suppression des doublons
print("Nombre de doublons apr√®s suppression:", df_assurance.duplicated().sum())

#V√©rification de l'existence des valeurs aberrantes
#Cr√©er des boxplots pour chaque colonne
# V√©rification de l'existence des valeurs aberrantes
# Cr√©er des boxplots uniquement pour les colonnes num√©riques
numeric_columns = df_assurance.select_dtypes(include=[np.number]).columns

print("Colonnes num√©riques d√©tect√©es:", list(numeric_columns))

for column in numeric_columns:
    if column != 'charges':  # Exclure la cible si n√©cessaire
        plt.figure(figsize=(8, 4))
        sns.boxplot(x=df_assurance[column])
        plt.title(f'Boxplot de {column}')
        plt.xlabel(column)
        plt.show()

# Calcul des statistiques pour BMI
Q1 = df_assurance['bmi'].quantile(0.25)
Q3 = df_assurance['bmi'].quantile(0.75)
IQR = Q3 - Q1

lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

print(f"Q1 (25√®me percentile): {Q1:.2f}")
print(f"Q3 (75√®me percentile): {Q3:.2f}")
print(f"IQR: {IQR:.2f}")
print(f"Borne inf√©rieure: {lower_bound:.2f}")
print(f"Borne sup√©rieure: {upper_bound:.2f}")

# Identifier les outliers
bmi_outliers = df_assurance[(df_assurance['bmi'] < lower_bound) | (df_assurance['bmi'] > upper_bound)]
print(f"\nNombre d'outliers dans BMI: {len(bmi_outliers)}")
print("\nValeurs outliers:")
print(bmi_outliers['bmi'].sort_values())

#Plage d'outliers : [47.41,53.13]
#On conserve les outliers car elles correspondent √† des cas d'ob√©sit√© s√©v√®re, qui sont m√©dicalement plausibles.

#Gestion des valeurs manquantes
#1-D√©tection des valeurs manquantes
df_assurance.isnull().sum()

#2-Imputation des valeurs manquantes
# 1. age - Imputation par la m√©diane
df_assurance['age'] = df_assurance['age'].fillna(df_assurance['age'].median())

# 2. sex - Imputation par le mode
df_assurance['sex'] = df_assurance['sex'].fillna(df_assurance['sex'].mode()[0])

# 3. bmi - Imputation par la m√©diane (robuste aux outliers)
df_assurance['bmi'] = df_assurance['bmi'].fillna(df_assurance['bmi'].median())

# 4. children - Imputation par le mode
df_assurance['children'] = df_assurance['children'].fillna(df_assurance['children'].mode()[0])

# 5. smoker - Imputation par le mode
df_assurance['smoker'] = df_assurance['smoker'].fillna(df_assurance['smoker'].mode()[0])

# 6. region - Imputation par le mode
df_assurance['region'] = df_assurance['region'].fillna(df_assurance['region'].mode()[0])

df_assurance.isnull().sum()

#Suppression des lignes contenant la valeur NULL pour la cible
df_assurance = df_assurance.dropna(subset=['charges'])
df_assurance.isnull().sum()

df_assurance.info()

#Partie 2: Transformation des donn√©es

#S√©paration des variables
X = df_assurance.drop("charges", axis=1)
y = df_assurance["charges"]

from sklearn.preprocessing import OneHotEncoder, MinMaxScaler
# Colonnes par type
categorical_cols = ['sex', 'smoker', 'region']
numeric_cols = ['age', 'bmi', 'children']

# 1. Encodage One-Hot des variables cat√©gorielles
encoder = OneHotEncoder(sparse_output=False)
one_hot_encoded = encoder.fit_transform(X[categorical_cols])

# Cr√©ation du DataFrame des variables encod√©es
encoded_df = pd.DataFrame(one_hot_encoded,
                         columns=encoder.get_feature_names_out(categorical_cols))

# 2. Normalisation des variables num√©riques
scaler = MinMaxScaler()
scaled_numeric = scaler.fit_transform(X[numeric_cols])

# Cr√©ation du DataFrame des variables normalis√©es
scaled_df = pd.DataFrame(scaled_numeric, columns=numeric_cols)

# 3. Combinaison des donn√©es transform√©es
X_transformed = pd.concat([scaled_df, encoded_df], axis=1)

# R√©sultat final
print(X_transformed.head())

#Importance des features sur les charges
from sklearn.ensemble import RandomForestRegressor


# 1. Cr√©er le mod√®le Random Forest
rf = RandomForestRegressor(n_estimators=100, random_state=42)

# 2. Entra√Æner le mod√®le sur les features transform√©es et la target
rf.fit(X_transformed, y)

# 3. R√©cup√©rer l'importance des features
feature_importances = pd.Series(rf.feature_importances_, index=X_transformed.columns)

# 4. Trier les features par importance d√©croissante
feature_importances = feature_importances.sort_values(ascending=False)

# 5. Afficher les r√©sultats
print(feature_importances)

"""# Objectif DS: Segmentation des assur√©s"""

# Mod√®le 1: KMeans

"""# Mod√®le 1: KMeans"""

# === Feature Engineering pour le reporting ===
df_assurance['bmi_category'] = pd.cut(df_assurance['bmi'],
                                     bins=[0, 18.5, 25, 30, 35, 100],
                                     labels=['Sous-poids', 'Normal', 'Surpoids', 'Ob√©sit√© I', 'Ob√©sit√© II'])
df_assurance['age_group'] = pd.cut(df_assurance['age'],
                                  bins=[0, 30, 45, 60, 100],
                                  labels=['Jeune', 'Adulte', 'Senior', '√Çg√©'])
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt
import numpy as np
# === M√©thode du coude + Score de silhouette ===
inertia = []
silhouette_scores = []
K_range = range(2, 6)

for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    cluster_labels = kmeans.fit_predict(X_transformed)

    inertia.append(kmeans.inertia_)
    silhouette_scores.append(silhouette_score(X_transformed, cluster_labels))

# Visualisation
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))

ax1.plot(K_range, inertia, marker='o')
ax1.set_xlabel('Nombre de clusters K')
ax1.set_ylabel('Inertia')
ax1.set_title('M√©thode du coude')

ax2.plot(K_range, silhouette_scores, marker='o', color='red')
ax2.set_xlabel('Nombre de clusters K')
ax2.set_ylabel('Score de Silhouette')
ax2.set_title('Score de Silhouette')

plt.tight_layout()
plt.show()

optimal_k = K_range[np.argmax(silhouette_scores)]
print(f"K optimal (silhouette): {optimal_k}")
print(f"Score de silhouette max: {max(silhouette_scores):.3f}")

kmeans_final = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)
clusters = kmeans_final.fit_predict(X_transformed)
df_assurance['cluster'] = clusters

# Noms commerciaux adapt√©s √† k=3
cluster_names_final = {
    0: "Fumeurs Actifs - Haut Risque",
    1: "Non-Fumeurs - Surpoids/Ob√©sit√© Mod√©r√©e",
    2: "Non-Fumeurs - Poids Normal/Surpoids L√©ger"
}
df_assurance['nom_cluster'] = df_assurance['cluster'].map(cluster_names_final)
cluster_packs_final = {}

# Profiling et reporting K-Means
for cluster_id in range(optimal_k):
    data = df_assurance[df_assurance['cluster']==cluster_id]
    name = cluster_names_final[cluster_id]
    smoker_pct = (data['smoker']=='yes').mean()*100
    avg_bmi = data['bmi'].mean()
    avg_charges = data['charges'].mean()

    if smoker_pct > 40:
        risk = "TR√àS √âLEV√â"; pack = "Premium Plus"
    elif smoker_pct > 10 or avg_bmi > 32:
        risk = "√âLEV√â"; pack = "Premium"
    elif avg_bmi > 28:
        risk = "MOD√âR√â"; pack = "Standard Plus"
    else:
        risk = "FAIBLE"; pack = "Standard"
       # Sauvegarde
    cluster_packs_final[cluster_id] = pack

    print(f"\nCluster {cluster_id} ({name}) - {len(data)} clients")
    print(f"  √Çge moyen: {data['age'].mean():.1f}, BMI moyen: {avg_bmi:.1f}, Enfants moyens: {data['children'].mean():.1f}")
    print(f"  % Fumeurs: {smoker_pct:.1f}%, R√©gion principale: {data['region'].mode()[0]}")
    print(f"  Charges moyennes: ${avg_charges:,.2f} (min {data['charges'].min():.0f}, max {data['charges'].max():.0f})")
    print(f"  Niveau de risque: {risk}, Pack: {pack}")

# Synth√®se globale K-Means
cluster_summary = df_assurance.groupby('cluster').agg({
    'age':'mean','bmi':'mean','children':'mean',
    'smoker': lambda x: (x=='yes').mean()*100,
    'charges':['mean','std','count']
}).round(2)
cluster_summary.columns = ['age_moyen','bmi_moyen','enfants_moyens','pourcent_fumeurs','charges_moyennes','ecart_type_charges','effectif']
cluster_summary['nom_cluster'] = cluster_summary.index.map(cluster_names_final)
print("\nSYNTH√àSE GLOBALE KMEANS")
print(cluster_summary)

# Palette de couleurs
colors = plt.cm.Set1.colors

# Cluster IDs et mapping couleur
cluster_ids = sorted(df_assurance['cluster'].unique())
color_map = {i: colors[i % len(colors)] for i in cluster_ids}

# Cr√©ation des sous-graphes
fig, axes = plt.subplots(2, 2, figsize=(16, 12))

# === 1. Charges vs √Çge par cluster ===
for cluster_id in cluster_ids:
    cluster_data = df_assurance[df_assurance['cluster'] == cluster_id]
    axes[0, 0].scatter(
        cluster_data['age'], cluster_data['charges'],
        color=color_map[cluster_id],
        label=cluster_names_final[cluster_id],
        alpha=0.7
    )
axes[0, 0].set_xlabel('√Çge', fontsize=12)
axes[0, 0].set_ylabel('Charges ($)', fontsize=12)
axes[0, 0].set_title('Charges vs √Çge par Cluster (KMeans)', fontsize=14)
axes[0, 0].legend(loc='best', fontsize=10)
axes[0, 0].grid(True, alpha=0.3)

# === 2. Charges vs BMI par cluster ===
for cluster_id in cluster_ids:
    cluster_data = df_assurance[df_assurance['cluster'] == cluster_id]
    axes[0, 1].scatter(
        cluster_data['bmi'], cluster_data['charges'],
        color=color_map[cluster_id],
        label=cluster_names_final[cluster_id],
        alpha=0.7
    )
axes[0, 1].set_xlabel('BMI', fontsize=12)
axes[0, 1].set_ylabel('Charges ($)', fontsize=12)
axes[0, 1].set_title('Charges vs BMI par Cluster (KMeans)', fontsize=14)
axes[0, 1].legend(loc='best', fontsize=10)
axes[0, 1].grid(True, alpha=0.3)

# === 3. Impact Fumeur sur les charges ===
smoker_charges = df_assurance.groupby(['cluster', 'smoker'])['charges'].mean().unstack()
smoker_charges = smoker_charges.rename(index=cluster_names_final)
smoker_charges.plot(
    kind='bar', ax=axes[1, 0],
    color=['#66c2a5', '#fc8d62'],
    width=0.7
)
axes[1, 0].set_title('Impact Fumeur sur Charges par Cluster (KMeans)', fontsize=14)
axes[1, 0].set_ylabel('Charges Moyennes ($)', fontsize=12)
axes[1, 0].tick_params(axis='x', rotation=45)
axes[1, 0].legend(title='Fumeur', fontsize=10)
axes[1, 0].grid(True, alpha=0.3)

# === 4. Charges moyennes par cluster avec √©cart-type ===
charges_stats = df_assurance.groupby('cluster')['charges'].agg(['mean', 'std'])
charges_stats.index = charges_stats.index.map(cluster_names_final)
axes[1, 1].bar(
    charges_stats.index,
    charges_stats['mean'],
    yerr=charges_stats['std'],
    capsize=5,
    color=[color_map[i] for i in cluster_ids],
    alpha=0.8
)
axes[1, 1].set_title('Charges Moyennes ¬± √âcart-Type par Cluster (KMeans)', fontsize=14)
axes[1, 1].set_ylabel('Charges ($)', fontsize=12)
axes[1, 1].tick_params(axis='x', rotation=45)
axes[1, 1].grid(True, axis='y', alpha=0.3)

plt.tight_layout()
plt.show()

#Mod√®le 2:AgglomerativeClustering

"""# Mod√®le 2: AgglomerativeClustering"""

from sklearn.cluster import AgglomerativeClustering

agglo = AgglomerativeClustering(n_clusters=optimal_k, linkage='ward')
clusters_hier = agglo.fit_predict(X_transformed)
df_assurance['cluster_hier'] = clusters_hier

# Noms commerciaux adapt√©s √† k=3
cluster_names_hier = {
    0: "Non-Fumeurs - Poids Normal/Surpoids L√©ger",
    1: "Fumeurs Intensifs - Haut Risque",
    2: "Non-Fumeurs - Surpoids/Ob√©sit√© Mod√©r√©e"
}
df_assurance['nom_cluster_hier'] = df_assurance['cluster_hier'].map(cluster_names_hier)
cluster_packs_hier = {}

# Profiling et reporting Agglomerative
for cluster_id in range(optimal_k):
    data = df_assurance[df_assurance['cluster_hier']==cluster_id]
    name = cluster_names_hier[cluster_id]
    smoker_pct = (data['smoker']=='yes').mean()*100
    avg_bmi = data['bmi'].mean()
    avg_charges = data['charges'].mean()

    if smoker_pct > 40:
        risk = "TR√àS √âLEV√â"; pack = "Premium Plus"
    elif smoker_pct > 10 or avg_bmi > 32:
        risk = "√âLEV√â"; pack = "Premium"
    elif avg_bmi > 28:
        risk = "MOD√âR√â"; pack = "Standard Plus"
    else:
        risk = "FAIBLE"; pack = "Standard"
    cluster_packs_hier[cluster_id] = pack

    print(f"\nCluster {cluster_id} ({name}) - {len(data)} clients")
    print(f"  √Çge moyen: {data['age'].mean():.1f}, BMI moyen: {avg_bmi:.1f}, Enfants moyens: {data['children'].mean():.1f}")
    print(f"  % Fumeurs: {smoker_pct:.1f}%, R√©gion principale: {data['region'].mode()[0]}")
    print(f"  Charges moyennes: ${avg_charges:,.2f} (min {data['charges'].min():.0f}, max {data['charges'].max():.0f})")
    print(f"  Niveau de risque: {risk}, Pack: {pack}")

# Synth√®se globale Agglomerative
cluster_summary_hier = df_assurance.groupby('cluster_hier').agg({
    'age':'mean','bmi':'mean','children':'mean',
    'smoker': lambda x: (x=='yes').mean()*100,
    'charges':['mean','std','count']
}).round(2)
cluster_summary_hier.columns = ['age_moyen','bmi_moyen','enfants_moyens','pourcent_fumeurs','charges_moyennes','ecart_type_charges','effectif']
cluster_summary_hier['nom_cluster'] = cluster_summary_hier.index.map(cluster_names_hier)
print("\nSYNTH√àSE GLOBALE AGGLOMERATIVE")
print(cluster_summary_hier)

# Palette de couleurs
colors = plt.cm.Set1.colors
cluster_ids = sorted(df_assurance['cluster_hier'].unique())
color_map = {i: colors[i % len(colors)] for i in cluster_ids}

# Cr√©ation des sous-graphes
fig, axes = plt.subplots(2, 2, figsize=(16, 12))

# === 1. Charges vs √Çge ===
for cluster_id in cluster_ids:
    cluster_data = df_assurance[df_assurance['cluster_hier'] == cluster_id]
    axes[0, 0].scatter(
        cluster_data['age'], cluster_data['charges'],
        color=color_map[cluster_id],
        label=cluster_names_hier[cluster_id],
        alpha=0.7
    )
axes[0, 0].set_xlabel('√Çge', fontsize=12)
axes[0, 0].set_ylabel('Charges ($)', fontsize=12)
axes[0, 0].set_title('Charges vs √Çge par Cluster (Hi√©rarchique)', fontsize=14)
axes[0, 0].legend(loc='best', fontsize=10)
axes[0, 0].grid(True, alpha=0.3)

# === 2. Charges vs BMI ===
for cluster_id in cluster_ids:
    cluster_data = df_assurance[df_assurance['cluster_hier'] == cluster_id]
    axes[0, 1].scatter(
        cluster_data['bmi'], cluster_data['charges'],
        color=color_map[cluster_id],
        label=cluster_names_hier[cluster_id],
        alpha=0.7
    )
axes[0, 1].set_xlabel('BMI', fontsize=12)
axes[0, 1].set_ylabel('Charges ($)', fontsize=12)
axes[0, 1].set_title('Charges vs BMI par Cluster (Hi√©rarchique)', fontsize=14)
axes[0, 1].legend(loc='best', fontsize=10)
axes[0, 1].grid(True, alpha=0.3)

# === 3. Impact Fumeur sur les charges ===
smoker_charges = df_assurance.groupby(['cluster_hier', 'smoker'])['charges'].mean().unstack()
smoker_charges = smoker_charges.rename(index=cluster_names_hier)
smoker_charges.plot(
    kind='bar', ax=axes[1, 0],
    color=['#66c2a5', '#fc8d62'],
    width=0.7
)
axes[1, 0].set_title('Impact Fumeur sur Charges par Cluster', fontsize=14)
axes[1, 0].set_ylabel('Charges Moyennes ($)', fontsize=12)
axes[1, 0].tick_params(axis='x', rotation=45)
axes[1, 0].legend(title='Fumeur', fontsize=10)

# === 4. Charges moyennes ¬± √©cart-type par cluster ===
charges_stats = df_assurance.groupby('cluster_hier')['charges'].agg(['mean', 'std']).rename(index=cluster_names_hier)
cluster_order = charges_stats.index.tolist()
colors_bars = [color_map[i] for i in cluster_ids]  # Associer couleur √† chaque cluster

axes[1, 1].bar(
    charges_stats.index,
    charges_stats['mean'],
    yerr=charges_stats['std'],
    capsize=5,
    color=colors_bars,
    alpha=0.8
)
axes[1, 1].set_title('Charges Moyennes ¬± √âcart-Type par Cluster', fontsize=14)
axes[1, 1].set_ylabel('Charges ($)', fontsize=12)
axes[1, 1].tick_params(axis='x', rotation=45)
axes[1, 1].grid(True, axis='y', alpha=0.3)

plt.tight_layout()
plt.show()

from sklearn.metrics import adjusted_rand_score

# Comparer la stabilit√© entre les deux m√©thodes
stability_score = adjusted_rand_score(clusters, clusters_hier)
print(f"Accord entre m√©thodes: {stability_score:.3f}")

from sklearn.metrics import silhouette_score

def comparer_silhouette(X, labels_kmeans, labels_agglom):
    """
    Compare le Silhouette Score entre KMeans et Agglomerative Clustering.

    Param√®tres:
        X : ndarray ou DataFrame
            Donn√©es transform√©es (features utilis√©es pour le clustering)
        labels_kmeans : array
            Labels pr√©dits par KMeans
        labels_agglom : array
            Labels pr√©dits par Agglomerative Clustering

    Retourne:
        meilleur_modele : str
            "KMeans" ou "Agglomerative Clustering"
    """

    # Calcul des scores
    silhouette_kmeans = silhouette_score(X, labels_kmeans)
    silhouette_agglo = silhouette_score(X, labels_agglom)

    print(f"\nüìä Silhouette KMeans: {silhouette_kmeans:.3f}")
    print(f"üìä Silhouette Agglomerative: {silhouette_agglo:.3f}")

    # S√©lection du meilleur mod√®le
    if silhouette_kmeans > silhouette_agglo:
        meilleur_modele = "KMeans"
        print("üéØ CONCLUSION : KMeans est le meilleur mod√®le (Silhouette plus √©lev√©)")
    else:
        meilleur_modele = "Agglomerative Clustering"
        print("üéØ CONCLUSION : Agglomerative Clustering est le meilleur mod√®le (Silhouette plus √©lev√©)")

    return meilleur_modele
# === SELECTION MODELE ===
meilleur_modele = comparer_silhouette(X_transformed, df_assurance['cluster'], df_assurance['cluster_hier'])

def predict_cluster(X_client_transformed):
    """Pr√©dit le cluster pour un nouveau client"""

    if meilleur_modele == "KMeans":
        cluster_id = kmeans_final.predict(X_client_transformed)[0]
        nom_cluster = cluster_names_kmeans[cluster_id] # type: ignore
    else:
        # Pour Agglomerative, on doit pr√©dire manuellement
        from sklearn.neighbors import NearestNeighbors
        # Trouve le cluster le plus proche parmi les donn√©es d'entra√Ænement
        nn = NearestNeighbors(n_neighbors=1)
        nn.fit(X_transformed)
        _, indices = nn.kneighbors(X_client_transformed)
        cluster_id = df_assurance.iloc[indices[0][0]]['cluster_hier']
        nom_cluster = cluster_names_hier[cluster_id]

    return cluster_id, nom_cluster, meilleur_modele

def preprocess_client_data(df_client):
    """
    Transforme les nouvelles donn√©es comme X_transformed :
    - Normalisation des colonnes num√©riques
    - One-Hot encoding des colonnes cat√©gorielles
    """
    # Colonnes
    numeric_cols = ['age','bmi','children']
    categorical_cols = ['sex','smoker','region']

    # 1. Encodage One-Hot
    one_hot = encoder.transform(df_client[categorical_cols])
    one_hot_df = pd.DataFrame(one_hot, columns=encoder.get_feature_names_out(categorical_cols))

    # 2. Normalisation
    scaled = scaler.transform(df_client[numeric_cols])
    scaled_df = pd.DataFrame(scaled, columns=numeric_cols)

    # 3. Combinaison
    X_client_transformed = pd.concat([scaled_df, one_hot_df], axis=1)

    return X_client_transformed

from sklearn.metrics import silhouette_score

def comparer_silhouette(X, labels_kmeans, labels_agglom):
    """
    Compare le Silhouette Score entre KMeans et Agglomerative Clustering.

    Param√®tres:
        X : ndarray ou DataFrame
            Donn√©es transform√©es (features utilis√©es pour le clustering)
        labels_kmeans : array
            Labels pr√©dits par KMeans
        labels_agglom : array
            Labels pr√©dits par Agglomerative Clustering

    Retourne:
        meilleur_modele : str
            "KMeans" ou "Agglomerative Clustering"
    """

    # Calcul des scores
    silhouette_kmeans = silhouette_score(X, labels_kmeans)
    silhouette_agglo = silhouette_score(X, labels_agglom)

    print(f"\nüìä Silhouette KMeans: {silhouette_kmeans:.3f}")
    print(f"üìä Silhouette Agglomerative: {silhouette_agglo:.3f}")

    # S√©lection du meilleur mod√®le
    if silhouette_kmeans > silhouette_agglo:
        meilleur_modele = "KMeans"
        print("üéØ CONCLUSION : KMeans est le meilleur mod√®le (Silhouette plus √©lev√©)")
    else:
        meilleur_modele = "Agglomerative Clustering"
        print("üéØ CONCLUSION : Agglomerative Clustering est le meilleur mod√®le (Silhouette plus √©lev√©)")

    return meilleur_modele
meilleur = comparer_silhouette(X_transformed, df_assurance['cluster'], df_assurance['cluster_hier'])

"""#Objectif DS: Construire les 2 mod√®les de r√©gression pour pr√©dire les frais m√©dicaux (charges) et comparer deux mod√®les : R√©gression Lin√©aire vs XGBoost"""

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import xgboost as xgb
from typing import Tuple, Dict, Any

#Pour de meilleurs visuels
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")

print(X_transformed.head())

X = X_transformed
y = df_assurance["charges"]
print("Dimensions de X :", X.shape)
print("Dimensions de y :", y.shape)

# Split Train/Test (une seule fois pour les deux mod√®les)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

print("Taille du jeu d‚Äôentra√Ænement :", X_train.shape)
print("Taille du jeu de test :", X_test.shape)

"""
*   Fonction evaluate_model va √©valuer un mod√®le et retourne les m√©triques

*   Visualisation les pr√©dictions vs valeurs r√©elles

*   Cr√©ation d'un DataFrame de comparaison √† partir d'un dictionnaire de m√©triques"""

def evaluate_model(y_true: np.ndarray, y_pred: np.ndarray, model_name: str) -> Dict[str, float]:
    """
    √âvalue un mod√®le et retourne les m√©triques
    """
    metrics = {
        'MAE': mean_absolute_error(y_true, y_pred),
        'RMSE': np.sqrt(mean_squared_error(y_true, y_pred)),
        'R2': r2_score(y_true, y_pred),
        'MAPE': np.mean(np.abs((y_true - y_pred) / y_true)) * 100  # Erreur en pourcentage
    }

    print(f"\nüìä √âvaluation {model_name} :")
    for metric, value in metrics.items():
        if metric == 'MAPE':
            print(f"{metric:5} : {value:.2f}%")
        else:
            print(f"{metric:5} : {value:.2f}")

    return metrics

def plot_predictions(y_true: np.ndarray, y_pred: np.ndarray, model_name: str):
    """
    Visualise les pr√©dictions vs valeurs r√©elles
    """
    fig, axes = plt.subplots(1, 2, figsize=(15, 6))

    # Graphique de dispersion
    axes[0].scatter(y_true, y_pred, alpha=0.6, s=50)
    axes[0].plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--', lw=2)
    axes[0].set_xlabel("Charges r√©elles")
    axes[0].set_ylabel("Charges pr√©dites")
    axes[0].set_title(f"{model_name} : Pr√©dictions vs R√©elles")
    axes[0].grid(True, alpha=0.3)

    # Distribution des erreurs
    errors = y_true - y_pred
    sns.histplot(errors, kde=True, ax=axes[1])
    axes[1].axvline(x=0, color='r', linestyle='--')
    axes[1].set_title(f"{model_name} : Distribution des erreurs")
    axes[1].set_xlabel("Erreur de pr√©diction")

    plt.tight_layout()
    plt.show()

    return errors

def create_comparison_df(metrics_dict: Dict[str, Dict]) -> pd.DataFrame:
    """
    Cr√©e un DataFrame de comparaison √† partir d'un dictionnaire de m√©triques
    """
    comparison_data = []
    for model_name, metrics in metrics_dict.items():
        row = {'Mod√®le': model_name}
        row.update(metrics)
        comparison_data.append(row)

    return pd.DataFrame(comparison_data)

"""# Mod√®le 1 : R√©gression Lin√©aire

"""

# Mod√®le 1 : R√©gression Lin√©aire

print("üîß Entra√Ænement du mod√®le de R√©gression Lin√©aire...")

# Construction et entra√Ænement du mod√®le
model_lr = LinearRegression()
model_lr.fit(X_train, y_train)

# Pr√©dictions
y_pred_lr = model_lr.predict(X_test)

# √âvaluation avec la fonction
metrics_lr = evaluate_model(y_test, y_pred_lr, "R√©gression Lin√©aire")

# Visualisations
errors_lr = plot_predictions(y_test, y_pred_lr, "R√©gression Lin√©aire")

# Informations suppl√©mentaires
print(f"\nüìà Coefficients du mod√®le lin√©aire : {len(model_lr.coef_)}")
print(f"üìç Intercept : {model_lr.intercept_:.2f}")

"""#Mod√®le 2 : XGBoost

"""

#Mod√®le 2 : XGBoost

print("\nüîß Entra√Ænement du mod√®le XGBoost...")

# Conversion en DMatrix
dtrain = xgb.DMatrix(X_train, label=y_train)
dtest = xgb.DMatrix(X_test, label=y_test)

# Param√®tres avec validation crois√©e implicite
params = {
    "objective": "reg:squarederror",
    "tree_method": "hist",
    "learning_rate": 0.1,
    "max_depth": 6,
    "subsample": 0.8,
    "colsample_bytree": 0.8,
    "reg_alpha": 0.1,  # L1 regularization
    "reg_lambda": 1.0, # L2 regularization
    "seed": 42
}

# Entra√Ænement avec callback de progression
evals = [(dtrain, "train"), (dtest, "validation")]

print("‚è≥ D√©but de l'entra√Ænement XGBoost...")
model_xgb = xgb.train(
    params=params,
    dtrain=dtrain,
    num_boost_round=1000,
    evals=evals,
    early_stopping_rounds=50,
    verbose_eval=50
)

# Pr√©dictions
y_pred_xgb = model_xgb.predict(dtest)

# √âvaluation
metrics_xgb = evaluate_model(y_test, y_pred_xgb, "XGBoost")

# Visualisations
errors_xgb = plot_predictions(y_test, y_pred_xgb, "XGBoost")

# Feature importance
plt.figure(figsize=(3, 8))
xgb.plot_importance(model_xgb, max_num_features=15, importance_type='weight')
plt.title("XGBoost - Importance des caract√©ristiques")
plt.tight_layout()
plt.show()

"""# Comparaison des 2 algorithmes

"""

# Cr√©ation du DataFrame de comparaison
metrics_dict = {
    "R√©gression Lin√©aire": metrics_lr,
    "XGBoost": metrics_xgb
}

results_df = create_comparison_df(metrics_dict)

print("\n" + "="*50)
print("üìä R√âSULTATS COMPARATIFS D√âTAILL√âS")
print("="*50)
print(results_df.round(4))

# Visualisation comparative am√©lior√©e
fig, axes = plt.subplots(2, 2, figsize=(8, 8))

# Graphique des m√©triques
metrics_to_plot = ['MAE', 'RMSE', 'R2', 'MAPE']
for idx, metric in enumerate(metrics_to_plot):
    ax = axes[idx//2, idx%2]
    sns.barplot(data=results_df, x='Mod√®le', y=metric, ax=ax, palette='viridis')
    ax.set_title(f'Comparaison - {metric}')
    ax.tick_params(axis='x', rotation=45)

    # Ajout des valeurs sur les barres
    for container in ax.containers:
        ax.bar_label(container, fmt='%.3f')

plt.tight_layout()
plt.show()

# Graphique des erreurs r√©siduelles
plt.figure(figsize=(10, 6))
plt.subplot(1, 2, 1)
sns.boxplot(data=[errors_lr, errors_xgb], palette='Set2')
plt.xticks([0, 1], ['R√©gression Lin√©aire', 'XGBoost'])
plt.title('Distribution des erreurs r√©siduelles')
plt.ylabel('Erreur')

plt.subplot(1, 2, 2)
sns.kdeplot(errors_lr, label='R√©gression Lin√©aire', fill=True)
sns.kdeplot(errors_xgb, label='XGBoost', fill=True)
plt.axvline(x=0, color='red', linestyle='--', alpha=0.5)
plt.title('Densit√© des erreurs')
plt.xlabel('Erreur')
plt.legend()

plt.tight_layout()
plt.show()

"""#Analyse de performance suppl√©mentaire entre les 2 algorithmes

"""

#Analyse de performance suppl√©mentaire

def analyze_residuals(y_true: np.ndarray, y_pred: np.ndarray, model_name: str):
    """Analyse approfondie des r√©sidus"""
    residuals = y_true - y_pred

    fig, axes = plt.subplots(2, 2, figsize=(12, 10))

    # R√©sidus vs pr√©dictions
    axes[0,0].scatter(y_pred, residuals, alpha=0.6)
    axes[0,0].axhline(y=0, color='red', linestyle='--')
    axes[0,0].set_xlabel('Pr√©dictions')
    axes[0,0].set_ylabel('R√©sidus')
    axes[0,0].set_title(f'{model_name} - R√©sidus vs Pr√©dictions')

    # QQ plot pour normalit√©
    from scipy import stats
    stats.probplot(residuals, dist="norm", plot=axes[0,1])
    axes[0,1].set_title(f'{model_name} - QQ Plot')

    # Autocorr√©lation des r√©sidus
    from statsmodels.tsa.stattools import acf
    acf_residuals = acf(residuals, nlags=40)
    axes[1,0].stem(acf_residuals)
    axes[1,0].set_title(f'{model_name} - Autocorr√©lation des r√©sidus')
    axes[1,0].set_xlabel('Lag')
    axes[1,0].set_ylabel('Autocorr√©lation')

    # Histogramme des r√©sidus
    axes[1,1].hist(residuals, bins=30, density=True, alpha=0.7)
    axes[1,1].set_xlabel('R√©sidus')
    axes[1,1].set_ylabel('Densit√©')
    axes[1,1].set_title(f'{model_name} - Distribution des r√©sidus')

    plt.tight_layout()
    plt.show()

# Application aux deux mod√®les
analyze_residuals(y_test, y_pred_lr, "R√©gression Lin√©aire")
analyze_residuals(y_test, y_pred_xgb, "XGBoost")

"""# Identification du meilleur mod√®le

"""

# Identification du meilleur mod√®le

# Access metrics from the dictionaries
mae_xgb = metrics_xgb['MAE']
mae_lr = metrics_lr['MAE']
r2_xgb = metrics_xgb['R2']
r2_lr = metrics_lr['R2']


if mae_xgb < mae_lr and r2_xgb > r2_lr:
    meilleur_modele = "XGBoost"
    modele_final = model_xgb
    print("üéØ CONCLUSION : XGBoest est s√©lectionn√© comme mod√®le final")
    print("   ‚úì Plus faible MAE (erreur moyenne)")
    print("   ‚úì Meilleur R¬≤ (explication de la variance)")
else:
    meilleur_modele = "R√©gression Lin√©aire"
    modele_final = model_lr
    print("üéØ CONCLUSION : R√©gression Lin√©aire est s√©lectionn√©e comme mod√®le final")

print(f"üìä Am√©lioration du MAE : {((mae_lr - mae_xgb) / mae_lr * 100):.1f}%")
print(f"üìä Am√©lioration du R¬≤ : {((r2_xgb - r2_lr) / r2_lr * 100):.1f}%")

"""#  PR√âDICTION SUR DE NOUVEAUX CLIENTS

"""

#  PR√âDICTION SUR DE NOUVEAUX CLIENTS

# 1. NOUVEAUX CLIENTS
nouveaux_clients = pd.DataFrame({
    'age': [25, 45, 60, 19, 35],
    'bmi': [22.5, 28.9, 33.1, 19.2, 26.5],
    'children': [0, 2, 1, 0, 2],
    'sex_female': [0, 1, 0, 1, 0],
    'sex_male': [1, 0, 1, 0, 1],
    'smoker_no': [1, 0, 0, 1, 1],
    'smoker_yes': [0, 1, 1, 0, 0],
    'region_northeast': [0, 1, 0, 0, 0],
    'region_northwest': [0, 0, 0, 1, 0],
    'region_southeast': [0, 0, 1, 0, 0],
    'region_southwest': [1, 0, 0, 0, 1]
})

print("üë• PROFILS DES NOUVEAUX CLIENTS :")
print(nouveaux_clients[['age', 'bmi', 'children', 'sex_male', 'smoker_yes']])

# 2. PR√âDICTION AVEC LE MEILLEUR MOD√àLE
predictions = modele_final.predict(xgb.DMatrix(nouveaux_clients))
nouveaux_clients['frais_predits'] = predictions

# 3. ANALYSE DES R√âSULTATS (NOUVEAU)
print("\n" + "="*60)
print("üí∞ R√âSULTATS DES PR√âDICTIONS - ANALYSE D√âTAILL√âE")
print("="*60)

for i in range(len(nouveaux_clients)):
    client = nouveaux_clients.iloc[i]

    # Cat√©gorisation du risque
    if client['smoker_yes'] == 1:
        risque = "üî¥ RISQUE √âLEV√â"
        motif = "(Fumeur)"
    elif client['bmi'] > 30:
        risque = "üü° RISQUE MOD√âR√â"
        motif = "(Ob√©sit√©)"
    elif client['age'] > 50:
        risque = "üü† RISQUE MOYEN"
        motif = "(√Çge avanc√©)"
    else:
        risque = "üü¢ RISQUE FAIBLE"
        motif = "(Profil sain)"

    print(f"üë§ Client {i+1}:")
    print(f"   ‚Ä¢ üìä Profil: {client['age']} ans, BMI {client['bmi']}, {client['children']} enfant(s)")
    print(f"   ‚Ä¢ üö¨ Fumeur: {'Oui' if client['smoker_yes'] == 1 else 'Non'}")
    print(f"   ‚Ä¢ üí∞ Frais pr√©dits: ${client['frais_predits']:,.2f}")
    print(f"   ‚Ä¢ ‚ö†Ô∏è  Niveau de risque: {risque} {motif}")
    print("-" * 50)

# 4. STATISTIQUES GLOBALES (NOUVEAU)
print("\nüìà STATISTIQUES GLOBALES DES PR√âDICTIONS :")
print(f"‚Ä¢ üìä Frais moyen: ${nouveaux_clients['frais_predits'].mean():,.2f}")
print(f"‚Ä¢ üìâ Frais minimum: ${nouveaux_clients['frais_predits'].min():,.2f}")
print(f"‚Ä¢ üìà Frais maximum: ${nouveaux_clients['frais_predits'].max():,.2f}")
print(f"‚Ä¢ üîÑ √âcart type: ${nouveaux_clients['frais_predits'].std():,.2f}")

# 5. VISUALISATION DES PR√âDICTIONS (NOUVEAU)
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
colors = ['red' if x == 1 else 'green' for x in nouveaux_clients['smoker_yes']]
plt.bar([f'Client {i+1}' for i in range(len(nouveaux_clients))],
        nouveaux_clients['frais_predits'], color=colors, alpha=0.7)
plt.title('Frais m√©dicaux pr√©dits par client\n(Rouge=Fumeur, Vert=Non-fumeur)')
plt.xticks(rotation=45)
plt.ylabel('Frais ($)')

plt.subplot(1, 2, 2)
plt.scatter(nouveaux_clients['age'], nouveaux_clients['frais_predits'],
           c=nouveaux_clients['bmi'], s=100, cmap='viridis')
plt.colorbar(label='BMI')
plt.xlabel('√Çge')
plt.ylabel('Frais pr√©dits ($)')
plt.title('Impact de l\'√¢ge et du BMI')

plt.tight_layout()
plt.show()

"""# Application Dash"""

# =============================================================================
# APPLICATION DASH OPTIMIS√âE - VERSION FINALE AVEC LIEN COLAB
# =============================================================================




"""**Classification**

*KNN*
"""

quantiles = df_assurance['charges'].quantile([0.33, 0.66]).values

def assign_reimbursement_class(charges):
    if charges <= quantiles[0]:
        return "R3"  # Faible charges ‚Üí fort remboursement
    elif charges <= quantiles[1]:
        return "R2"  # Charges moyennes ‚Üí remboursement moyen
    else:
        return "R1"  # Charges √©lev√©es ‚Üí faible remboursement

df_assurance['remboursement_class'] = df_assurance['charges'].apply(assign_reimbursement_class)

print("R√©partition des classes :")
print(df_assurance['remboursement_class'].value_counts())

X = X_transformed.values  # tes features d√©j√† encod√©es + normalis√©es
y = df_assurance['remboursement_class'].values  # ta cible

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

knn = KNeighborsClassifier(n_neighbors=5)

knn.fit(X_train, y_train)

y_pred = knn.predict(X_test)

print("Rapport de classification (KNN k=5):\n")
print(classification_report(y_test, y_pred))

cm = confusion_matrix(y_test, y_pred, labels=["R1","R2","R3"])
ConfusionMatrixDisplay(cm, display_labels=["R1","R2","R3"]).plot(cmap=plt.cm.Blues)
plt.title("Matrice de confusion - KNN (k=5)")
plt.show()

# 5) (Optionnel) Chercher le meilleur k
scores = []
ks = range(1, 21)
for k in ks:
    m = KNeighborsClassifier(n_neighbors=k)
    m.fit(X_train, y_train)
    scores.append(m.score(X_test, y_test))

plt.figure()
plt.plot(ks, scores, marker='o')
plt.xlabel("Nombre de voisins (k)")
plt.ylabel("Accuracy sur test")
plt.title("Choix du meilleur k (avec X_transformed)")
plt.show()

"""Arbre

"""

X = X_transformed   # Ton jeu de donn√©es encod√© et normalis√©
y = df_assurance["remboursement_class"]

# V√©rifier les dimensions
X.shape, y.shape

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# --- 3. Entra√Æner l‚Äôarbre de d√©cision
clf = DecisionTreeClassifier(
    max_depth=5,
    class_weight={'R1':1, 'R2':1, 'R3':1.5}, # pond√©ration des classes
    random_state=42
)
clf.fit(X_train, y_train)

# Pr√©dictions
y_pred = clf.predict(X_test)

# Matrice de confusion
print("Matrice de confusion :\n", confusion_matrix(y_test, y_pred))

# Rapport de classification
print("\nRapport de classification :\n", classification_report(y_test, y_pred))

ConfusionMatrixDisplay.from_estimator(clf, X_test, y_test, cmap='Blues')
plt.show()

import seaborn as sns
from sklearn.metrics import classification_report

report = classification_report(y_test, y_pred, output_dict=True)
df_report = pd.DataFrame(report).iloc[:-1, :3]  # exclut accuracy

sns.heatmap(df_report, annot=True, cmap="Blues")
plt.title("Precision / Recall / F1-score")
plt.show()

encoded_cols = encoder.get_feature_names_out(categorical_cols)
all_features = numeric_cols + list(encoded_cols)

plt.figure(figsize=(20,10))
plot_tree(clf, feature_names=all_features, class_names=['R1','R2','R3'], filled=True)
plt.show()


#afficahge 

# =============================================================================
# SAUVEGARDE DES MOD√àLES POUR FLASK
# =============================================================================

# Sauvegarde des mod√®les
import joblib
import os
os.makedirs('models', exist_ok=True)
joblib.dump(modele_final, 'models/modele_final.pkl')
joblib.dump(encoder, 'models/encoder.pkl')
joblib.dump(scaler, 'models/scaler.pkl')
joblib.dump(clf, 'models/clf.pkl')
print("‚úÖ Mod√®les sauvegard√©s!")


###

# =============================================================================
# SAUVEGARDE DES MOD√àLES POUR FLASK
# =============================================================================

def sauvegarder_modeles():
    """Sauvegarde tous les mod√®les n√©cessaires pour l'application Flask"""
    import joblib
    import os
    
    # Cr√©er le dossier models s'il n'existe pas
    os.makedirs('models', exist_ok=True)
    
    try:
        # V√©rifier que les mod√®les existent
        required_models = {
            'modele_final': modele_final,
            'encoder': encoder,
            'scaler': scaler,
            'clf': clf
        }
        
        for name, model in required_models.items():
            if model is None:
                print(f"‚ùå Le mod√®le {name} n'est pas d√©fini")
                return False
        
        # Sauvegarder les mod√®les et transformateurs
        joblib.dump(modele_final, 'models/modele_final.pkl')
        joblib.dump(encoder, 'models/encoder.pkl')
        joblib.dump(scaler, 'models/scaler.pkl')
        joblib.dump(clf, 'models/clf.pkl')
        
        print("‚úÖ Mod√®les sauvegard√©s avec succ√®s dans le dossier 'models/'")
        print("üìÅ Fichiers cr√©√©s :")
        print("   - modele_final.pkl")
        print("   - encoder.pkl") 
        print("   - scaler.pkl")
        print("   - clf.pkl")
        return True
        
    except Exception as e:
        print(f"‚ùå Erreur lors de la sauvegarde : {e}")
        return False

# Ex√©cuter la sauvegarde si ce script est ex√©cut√© directement
if __name__ == "__main__":
    # Attendre que tous les mod√®les soient cr√©√©s dans votre code principal
    # Cette partie doit √™tre appel√©e apr√®s l'entra√Ænement de tous les mod√®les
    
    # Pour l'instant, ex√©cutons juste le nettoyage et l'analyse
    print("üîç Analyse des donn√©es termin√©e!")
    
    # Si vous voulez sauvegarder imm√©diatement, d√©commentez la ligne suivante :
    # sauvegarder_modeles()